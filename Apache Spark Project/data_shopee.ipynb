{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hispanic-assault",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "super-wages",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/notebook\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "twelve-frederick",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.pyspark_setup import get_spark_session\n",
    "from ipynb.fs.full.conf_template import Struct as Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "formal-blond",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "spark = get_spark_session(\"data_bc\", swan_spark_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "pointed-intention",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import RobustScaler\n",
    "from pyspark.ml.feature import MaxAbsScaler\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, BucketedRandomProjectionLSH, VectorSlicer\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "from pyspark.ml.linalg import Vectors,VectorUDT\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import random\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-african",
   "metadata": {},
   "source": [
    "# Receiving data from BE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "improved-niagara",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kafka import KafkaConsumer\n",
    "# from json import loads\n",
    "# import json\n",
    "# from decimal import Decimal\n",
    "# import iso8601\n",
    "# from threading import Thread\n",
    "# import pika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "chicken-enough",
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema_shopee = StructType([\n",
    "#     StructField('ordersn', StringType(), True),\n",
    "#     StructField('order_status', StringType(), True),\n",
    "#     StructField('shopid', IntegerType(), True),\n",
    "#     StructField('create_time', TimestampType(), True),\n",
    "#     StructField('pay_time', TimestampType(), True),\n",
    "#     StructField('update_time', TimestampType(), True),\n",
    "#     StructField('buyer_username', StringType(), True),\n",
    "#     StructField('recipient_name', StringType(), True),\n",
    "#     StructField('recipient_phone', StringType(), True),\n",
    "#     StructField('recipient_address', StringType(), True),\n",
    "#     StructField('recipient_district', StringType(), True),\n",
    "#     StructField('recipient_city', StringType(), True),\n",
    "#     StructField('recipient_province', StringType(), True),\n",
    "#     StructField('timestamp', TimestampType(), True),\n",
    "#     StructField('estimated_shipping_fee', DecimalType(38,18), True),\n",
    "# ])\n",
    "\n",
    "# schema_bc = StructType([\n",
    "#     StructField('brand', StringType(), True),\n",
    "#     StructField('order_id', LongType(), True),\n",
    "#     StructField('order_status', StringType(), True),\n",
    "#     StructField('date_order', TimestampType(), True),\n",
    "#     StructField('customer_id', StringType(), True),\n",
    "#     StructField('payment_method', StringType(), True),\n",
    "#     StructField('date_completed', TimestampType(), True),\n",
    "#     StructField('date_paid', TimestampType(), True),\n",
    "#     StructField('date_invoice', TimestampType(), True),\n",
    "#     StructField('no_invoice', StringType(), True),\n",
    "#     StructField('order_currency', StringType(), True),\n",
    "#     StructField('order_subtotal', DecimalType(38,18), True),\n",
    "#     StructField('cart_discount', DecimalType(38,18), True),\n",
    "#     StructField('redeemed_point', DecimalType(38,18), True),\n",
    "#     StructField('order_shipping', DecimalType(38,18), True),\n",
    "#     StructField('order_total', DecimalType(38,18), True),\n",
    "#     StructField('coupon_usage', StringType(), True),\n",
    "#     StructField('order_stock_reduced', StringType(), True),\n",
    "#     StructField('xendit_invoice', StringType(), True),\n",
    "#     StructField('version', StringType(), True),\n",
    "#     StructField('write_date', TimestampType(), True)\n",
    "# ])\n",
    "\n",
    "# tmp_shopee = []\n",
    "# tmp_bc = []\n",
    "\n",
    "# consumer_bc = KafkaConsumer(\n",
    "#     'com.xyz.bc.transfer',\n",
    "#      bootstrap_servers=['kafka.naufalhilmi.svc.cluster.local:9092'],\n",
    "#      group_id = \"xyz\",\n",
    "# #      consumer_timeout_ms = 60000,\n",
    "#      enable_auto_commit=True,\n",
    "#      value_deserializer=lambda x: loads(x.decode('utf-8')))\n",
    "\n",
    "# consumer_shopee = KafkaConsumer(\n",
    "#     'com.xyz.shopee.transfer',\n",
    "#      bootstrap_servers=['kafka.naufalhilmi.svc.cluster.local:9092'],\n",
    "#      group_id = \"xyz\",\n",
    "# #      consumer_timeout_ms = 60000,\n",
    "#      enable_auto_commit=True,\n",
    "#      value_deserializer=lambda x: loads(x.decode('utf-8')))\n",
    "\n",
    "# def recieve_bc(consumer):\n",
    "#     for message_bc in consumer:\n",
    "#         message_bc = message_bc.value\n",
    "#         lst_timestmp_bc = ['date_order', 'date_completed', 'date_paid', 'date_invoice', 'write_date']\n",
    "#         lst_decimal_bc = ['order_subtotal', 'cart_discount', 'redeemed_point', 'order_shipping', 'order_total']\n",
    "#         for i in lst_decimal_bc:\n",
    "#             if message_bc[i] == None:\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 message_bc[i] = Decimal(message_bc[i])\n",
    "            \n",
    "#         for i in lst_timestmp_bc:\n",
    "#             if message_bc[i] == None:\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 message_bc[i] = iso8601.parse_date(message_bc[i], default_timezone=None)\n",
    "#         tmp_bc.append(message_bc)\n",
    "\n",
    "# def recieve_shopee(consumer):\n",
    "#     for message_shopee in consumer:\n",
    "#         message_shopee = message_shopee.value\n",
    "#         lst_timestmp_shopee = ['create_time', 'pay_time', 'update_time', 'timestamp']\n",
    "#         message_shopee['estimated_shipping_fee'] = Decimal(message_shopee['estimated_shipping_fee'])\n",
    "            \n",
    "#         for i in lst_timestmp_shopee:\n",
    "#             if message_shopee[i] == None:\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 message_shopee[i] = iso8601.parse_date(message_shopee[i], default_timezone=None)\n",
    "#         tmp_shopee.append(message_shopee)\n",
    "\n",
    "# threads = list()\n",
    "        \n",
    "# bc_thread = Thread(target=recieve_bc, args=(consumer_bc,))\n",
    "# shopee_thread = Thread(target=recieve_shopee, args=(consumer_shopee,))\n",
    "\n",
    "# threads.append(bc_thread)\n",
    "# threads.append(shopee_thread)\n",
    "\n",
    "# for i in threads:\n",
    "#     i.start()\n",
    "\n",
    "# for i in threads:\n",
    "#     i.join()\n",
    "\n",
    "# df_recieve_bc = spark.createDataFrame(tmp_bc,schema_bc)        \n",
    "# df_recieve_shopee = spark.createDataFrame(tmp_shopee,schema_shopee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "oriental-apartment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema_shopee = StructType([\n",
    "#     StructField('ordersn', StringType(), True),\n",
    "#     StructField('order_status', StringType(), True),\n",
    "#     StructField('shopid', IntegerType(), True),\n",
    "#     StructField('create_time', TimestampType(), True),\n",
    "#     StructField('pay_time', TimestampType(), True),\n",
    "#     StructField('update_time', TimestampType(), True),\n",
    "#     StructField('buyer_username', StringType(), True),\n",
    "#     StructField('recipient_name', StringType(), True),\n",
    "#     StructField('recipient_phone', StringType(), True),\n",
    "#     StructField('recipient_address', StringType(), True),\n",
    "#     StructField('recipient_district', StringType(), True),\n",
    "#     StructField('recipient_city', StringType(), True),\n",
    "#     StructField('recipient_province', StringType(), True),\n",
    "#     StructField('timestamp', TimestampType(), True),\n",
    "#     StructField('estimated_shipping_fee', DecimalType(38,18), True),\n",
    "# ])\n",
    "\n",
    "# schema_bc = StructType([\n",
    "#     StructField('brand', StringType(), True),\n",
    "#     StructField('order_id', LongType(), True),\n",
    "#     StructField('order_status', StringType(), True),\n",
    "#     StructField('date_order', TimestampType(), True),\n",
    "#     StructField('customer_id', StringType(), True),\n",
    "#     StructField('payment_method', StringType(), True),\n",
    "#     StructField('date_completed', TimestampType(), True),\n",
    "#     StructField('date_paid', TimestampType(), True),\n",
    "#     StructField('date_invoice', TimestampType(), True),\n",
    "#     StructField('no_invoice', StringType(), True),\n",
    "#     StructField('order_currency', StringType(), True),\n",
    "#     StructField('order_subtotal', DecimalType(38,18), True),\n",
    "#     StructField('cart_discount', DecimalType(38,18), True),\n",
    "#     StructField('redeemed_point', DecimalType(38,18), True),\n",
    "#     StructField('order_shipping', DecimalType(38,18), True),\n",
    "#     StructField('order_total', DecimalType(38,18), True),\n",
    "#     StructField('coupon_usage', StringType(), True),\n",
    "#     StructField('order_stock_reduced', StringType(), True),\n",
    "#     StructField('xendit_invoice', StringType(), True),\n",
    "#     StructField('version', StringType(), True),\n",
    "#     StructField('write_date', TimestampType(), True)\n",
    "# ])\n",
    "\n",
    "# tmp_shopee = []\n",
    "# tmp_bc = []\n",
    "\n",
    "# class ThreadedConsumer(Thread):\n",
    "#     def __init__(self, RABBIT_URL, QUEUE_NAME):\n",
    "#         Thread.__init__(self)\n",
    "#         self.queue_name = QUEUE_NAME\n",
    "#         parameters = pika.URLParameters(RABBIT_URL)\n",
    "#         connection = pika.BlockingConnection(parameters)\n",
    "#         self.channel = connection.channel()\n",
    "#         # self.channel.queue_declare(queue=QUEUE_NAME, auto_delete=False)\n",
    "#         # self.channel.queue_bind(queue=QUEUE_NAME, exchange=EXCHANGE, routing_key=ROUTING_KEY)\n",
    "#         # self.channel.basic_qos(prefetch_count=THREADS*10)\n",
    "#         self.channel.basic_consume(QUEUE_NAME, on_message_callback=self.callback)\n",
    "#         Thread(target=self.channel.basic_consume(QUEUE_NAME, on_message_callback=self.callback))\n",
    "        \n",
    "#     def callback(self, channel, method, properties, body):\n",
    "#         message = json.loads(body)\n",
    "#         if self.queue_name == \"queue.brandcommerce\" :\n",
    "#             lst_timestmp_bc = ['date_order', 'date_completed', 'date_paid', 'date_invoice', 'write_date']\n",
    "#             lst_decimal_bc = ['order_subtotal', 'cart_discount', 'redeemed_point', 'order_shipping', 'order_total']\n",
    "#             for i in lst_decimal_bc:\n",
    "#                 if message[i] == None:\n",
    "#                     continue\n",
    "#                 else:\n",
    "#                     message[i] = Decimal(message[i])\n",
    "\n",
    "#             for i in lst_timestmp_bc:\n",
    "#                 if message[i] == None:\n",
    "#                     continue\n",
    "#                 else:\n",
    "#                     message[i] = iso8601.parse_date(message[i], default_timezone=None)\n",
    "#             tmp_bc.append(message)\n",
    "#             channel.basic_ack(delivery_tag=method.delivery_tag)\n",
    "#         else:\n",
    "#             lst_timestmp_shopee = ['create_time', 'pay_time', 'update_time', 'timestamp']\n",
    "#             message['estimated_shipping_fee'] = Decimal(message['estimated_shipping_fee'])\n",
    "\n",
    "#             for i in lst_timestmp_shopee:\n",
    "#                 if message[i] == None:\n",
    "#                     continue\n",
    "#                 else:\n",
    "#                     message[i] = iso8601.parse_date(message[i], default_timezone=None)\n",
    "#             tmp_shopee.append(message)\n",
    "#             channel.basic_ack(delivery_tag=method.delivery_tag)\n",
    "\n",
    "#     def run(self):\n",
    "#         print ('starting thread to consume from rabbit...')\n",
    "#         self.channel.start_consuming()\n",
    "\n",
    "# thread_list = []\n",
    "# thread_list.append(ThreadedConsumer('amqp://user:user@rabbitmq-0.rabbitmq-headless.naufalhilmi.svc.cluster.local:5672', \"queue.brandcommerce\"))\n",
    "# thread_list.append(ThreadedConsumer('amqp://user:user@rabbitmq-1.rabbitmq-headless.naufalhilmi.svc.cluster.local:5672', \"queue.shopee\"))\n",
    "# for i in thread_list:\n",
    "#     i.start()\n",
    "\n",
    "# for i in thread_list:\n",
    "#     i.join()\n",
    "\n",
    "# df_recieve_bc = spark.createDataFrame(tmp_bc,schema_bc)        \n",
    "# df_recieve_shopee = spark.createDataFrame(tmp_shopee,schema_shopee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "continental-player",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_recieve_bc.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "limiting-auckland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_recieve_shopee.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "crude-episode",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_recieve_bc.write.format(\"jdbc\")\\\n",
    "# .option(\"url\",'jdbc:postgresql://postgres-db-lb.naufalhilmi.svc.cluster.local:5432/postgres')\\\n",
    "# .option(\"dbtable\",'brand_commerce.order_transaction_bc_naufal')\\\n",
    "# .option(\"user\",'postgres')\\\n",
    "# .option(\"password\",'postgres')\\\n",
    "# .option(\"driver\",\"org.postgresql.Driver\")\\\n",
    "# .mode('append').save()\n",
    "\n",
    "# df_recieve_shopee.write.format(\"jdbc\")\\\n",
    "# .option(\"url\",'jdbc:postgresql://postgres-db-lb.naufalhilmi.svc.cluster.local:5432/postgres')\\\n",
    "# .option(\"dbtable\",'shopee.order_transaction_shopee_naufal')\\\n",
    "# .option(\"user\",'postgres')\\\n",
    "# .option(\"password\",'postgres')\\\n",
    "# .option(\"driver\",\"org.postgresql.Driver\")\\\n",
    "# .mode('append').save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-minute",
   "metadata": {},
   "source": [
    "# Postgres Data Brand_Commerce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "auburn-polish",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_shipping_bc = spark.read.format(\"jdbc\").\\\n",
    "options(\n",
    "         url='jdbc:postgresql://postgres-db-lb.naufalhilmi.svc.cluster.local:5432/postgres',\n",
    "         dbtable='brand_commerce.order_shipping',\n",
    "         user='postgres',\n",
    "         password='postgres',\n",
    "         driver='org.postgresql.Driver').\\\n",
    "load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "distant-distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_product_bc = spark.read.format(\"jdbc\").\\\n",
    "options(\n",
    "         url='jdbc:postgresql://postgres-db-lb.naufalhilmi.svc.cluster.local:5432/postgres',\n",
    "         dbtable='brand_commerce.order_product',\n",
    "         user='postgres',\n",
    "         password='postgres',\n",
    "         driver='org.postgresql.Driver').\\\n",
    "load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "passive-celtic",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_bc = spark.read.format(\"jdbc\").\\\n",
    "options(\n",
    "         url='jdbc:postgresql://postgres-db-lb.naufalhilmi.svc.cluster.local:5432/postgres',\n",
    "         dbtable='brand_commerce.order_transaction_bc_naufal',\n",
    "         user='postgres',\n",
    "         password='postgres',\n",
    "         driver='org.postgresql.Driver').\\\n",
    "load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-lewis",
   "metadata": {},
   "source": [
    "# Postgres Data Shopee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "terminal-roman",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_shopee = spark.read.format(\"jdbc\").\\\n",
    "options(\n",
    "         url='jdbc:postgresql://postgres-db-lb.naufalhilmi.svc.cluster.local:5432/postgres',\n",
    "         dbtable='shopee.order_transaction_shopee_naufal',\n",
    "         user='postgres',\n",
    "         password='postgres',\n",
    "         driver='org.postgresql.Driver').\\\n",
    "load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "supposed-botswana",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_product_shopee = spark.read.format(\"jdbc\").\\\n",
    "options(\n",
    "         url='jdbc:postgresql://postgres-db-lb.naufalhilmi.svc.cluster.local:5432/postgres',\n",
    "         dbtable='shopee.income_item_detail',\n",
    "         user='postgres',\n",
    "         password='postgres',\n",
    "         driver='org.postgresql.Driver').\\\n",
    "load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fifth-globe",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_bc = ['wc-closed', 'accepted', 'Completed', 'completed', 'wc-completed', 'Waiting Payment', 'Under Shipment']\n",
    "lst_shopee = ['READY_TO_SHIP', 'COMPLETED', 'SHIPPED', 'TO_CONFIRM_RECEIVE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "young-palmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_bc = order_bc.filter(col('order_status').isin(lst_bc))\n",
    "order_bc = order_bc.filter(col(\"date_paid\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "deadly-seeker",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_shopee = order_shopee.filter(col('order_status').isin(lst_shopee))\n",
    "order_shopee = order_shopee.filter(col(\"pay_time\").isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "major-spider",
   "metadata": {},
   "source": [
    "# Preprocessing raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "automotive-prince",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update order_id in order_billing data to match the brand\n",
    "order_shipping_bc = order_shipping_bc\\\n",
    "      .withColumn(\"shipping_province\", when(upper(col('shipping_province')).contains('JAWA BARAT'),'Jawa Barat') \\\n",
    "      .when(upper(col('shipping_province')).contains('DKI JAKARTA'),'DKI Jakarta') \\\n",
    "      .when(upper(col('shipping_province')).contains('BANTEN'),'Banten') \\\n",
    "      .when(upper(col('shipping_province')).contains('JAWA TENGAH'),'Jawa Tengah') \\\n",
    "      .when(upper(col('shipping_province')).contains('JAWA TIMUR'),'Jawa Timur') \\\n",
    "      .when(upper(col('shipping_province')).contains('SUMATERA UTARA'),'Sumatera Utara') \\\n",
    "      .otherwise('Other'))\\\n",
    "      .withColumn(\"order_id\", when(order_shipping_bc.brand == \"Emina\",concat(col('order_id'), lit(\"E\"))) \\\n",
    "      .when(order_shipping_bc.brand == \"Make Over\",concat(col('order_id'), lit(\"M\"))) \\\n",
    "      .when(order_shipping_bc.brand == \"Wardah\",concat(col('order_id'), lit(\"W\"))) \\\n",
    "      .when(order_shipping_bc.brand == \"Kahf\",concat(col('order_id'), lit(\"K\"))) \\\n",
    "      .when(order_shipping_bc.brand == \"Crystallure\",concat(col('order_id'), lit(\"C\"))) \\\n",
    "      .otherwise(order_shipping_bc.order_id))\n",
    "    \n",
    "order_product_bc = order_product_bc\\\n",
    "      .withColumn(\"order_id\", when(order_product_bc.brand == \"Emina\",concat(col('order_id'), lit(\"E\"))) \\\n",
    "      .when(order_product_bc.brand == \"Make Over\",concat(col('order_id'), lit(\"M\"))) \\\n",
    "      .when(order_product_bc.brand == \"Wardah\",concat(col('order_id'), lit(\"W\"))) \\\n",
    "      .when(order_product_bc.brand == \"Kahf\",concat(col('order_id'), lit(\"K\"))) \\\n",
    "      .when(order_product_bc.brand == \"Crystallure\",concat(col('order_id'), lit(\"C\"))) \\\n",
    "      .otherwise(order_product_bc.order_id))\\\n",
    "      .withColumn(\"order_item_id\", when(order_product_bc.brand == \"Emina\",concat(col('order_item_id'), lit(\"E\"))) \\\n",
    "      .when(order_product_bc.brand == \"Make Over\",concat(col('order_item_id'), lit(\"M\"))) \\\n",
    "      .when(order_product_bc.brand == \"Wardah\",concat(col('order_item_id'), lit(\"W\"))) \\\n",
    "      .when(order_product_bc.brand == \"Kahf\",concat(col('order_item_id'), lit(\"K\"))) \\\n",
    "      .when(order_product_bc.brand == \"Crystallure\",concat(col('order_item_id'), lit(\"C\"))) \\\n",
    "      .otherwise(order_product_bc.order_item_id))\\\n",
    "      .withColumn(\"product_id\", when(order_product_bc.brand == \"Emina\",concat(col('product_id'), lit(\"E\"))) \\\n",
    "      .when(order_product_bc.brand == \"Make Over\",concat(col('product_id'), lit(\"M\"))) \\\n",
    "      .when(order_product_bc.brand == \"Wardah\",concat(col('product_id'), lit(\"W\"))) \\\n",
    "      .when(order_product_bc.brand == \"Kahf\",concat(col('product_id'), lit(\"K\"))) \\\n",
    "      .when(order_product_bc.brand == \"Crystallure\",concat(col('product_id'), lit(\"C\"))) \\\n",
    "      .otherwise(order_product_bc.product_id))\n",
    "\n",
    "order_bc = order_bc\\\n",
    "      .withColumn(\"customer_id\", when(order_bc.brand == \"Emina\",concat(col('customer_id'), lit(\"-E-bc\"))) \\\n",
    "      .when(order_bc.brand == \"Make Over\",concat(col('customer_id'), lit(\"-M-bc\"))) \\\n",
    "      .when(order_bc.brand == \"Wardah\",concat(col('customer_id'), lit(\"-W-bc\"))) \\\n",
    "      .when(order_bc.brand == \"Kahf\",concat(col('customer_id'), lit(\"-K-bc\"))) \\\n",
    "      .when(order_bc.brand == \"Crystallure\",concat(col('customer_id'), lit(\"-C-bc\"))) \\\n",
    "      .otherwise(order_bc.customer_id))\\\n",
    "      .withColumn(\"order_id\", when(order_bc.brand == \"Emina\",concat(col('order_id'), lit(\"E\"))) \\\n",
    "      .when(order_bc.brand == \"Make Over\",concat(col('order_id'), lit(\"M\"))) \\\n",
    "      .when(order_bc.brand == \"Wardah\",concat(col('order_id'), lit(\"W\"))) \\\n",
    "      .when(order_bc.brand == \"Kahf\",concat(col('order_id'), lit(\"K\"))) \\\n",
    "      .when(order_bc.brand == \"Crystallure\",concat(col('order_id'), lit(\"C\"))) \\\n",
    "      .otherwise(order_bc.order_id))\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "answering-fleece",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_shopee = order_shopee\\\n",
    "      .withColumn(\"recipient_province\", when(upper(col('recipient_province')).contains('JAWA BARAT'),'Jawa Barat') \\\n",
    "      .when(upper(col('recipient_province')).contains('DKI JAKARTA'),'DKI Jakarta') \\\n",
    "      .when(upper(col('recipient_province')).contains('BANTEN'),'Banten') \\\n",
    "      .when(upper(col('recipient_province')).contains('JAWA TENGAH'),'Jawa Tengah') \\\n",
    "      .when(upper(col('recipient_province')).contains('JAWA TIMUR'),'Jawa Timur') \\\n",
    "      .when(upper(col('recipient_province')).contains('SUMATERA UTARA'),'Sumatera Utara') \\\n",
    "      .otherwise('Other'))\\\n",
    "      .withColumn(\"buyer_username\", when(order_shopee.shopid == 59763733, concat(col('buyer_username'), lit(\"-W-shopee\"))) \\\n",
    "      .when(order_shopee.shopid == 575053680, concat(col('buyer_username'), lit(\"-B-shopee\"))) \\\n",
    "      .when(order_shopee.shopid == 637555425, concat(col('buyer_username'), lit(\"-T-shopee\"))) \\\n",
    "      .when(order_shopee.shopid == 63983008, concat(col('buyer_username'), lit(\"-E-shopee\"))) \\\n",
    "      .when(order_shopee.shopid == 326487418, concat(col('buyer_username'), lit(\"-K-shopee\"))) \\\n",
    "      .when(order_shopee.shopid == 524963178, concat(col('buyer_username'), lit(\"-L-shopee\"))) \\\n",
    "      .when(order_shopee.shopid == 63984475, concat(col('buyer_username'), lit(\"-M-shopee\"))) \\\n",
    "      .when(order_shopee.shopid == 401724234, concat(col('buyer_username'), lit(\"-O-shopee\"))) \\\n",
    "      .when(order_shopee.shopid == 652866307, concat(col('buyer_username'), lit(\"-I-shopee\"))) \\\n",
    "      .when(order_shopee.shopid == 625116419, concat(col('buyer_username'), lit(\"-C-shopee\"))) \\\n",
    "      .otherwise(order_shopee.shopid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-access",
   "metadata": {},
   "source": [
    "# Extract feature for ML for Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "productive-atlas",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_shopee():\n",
    "    order_shopee_tmp = order_shopee.select('ordersn', 'create_time', 'buyer_username', 'recipient_province')\n",
    "    \n",
    "    predict_month = order_shopee_tmp.agg(trunc(max(col('create_time')),'month')).collect()[0][0]\n",
    "    \n",
    "    min_month = predict_month + relativedelta(months=-6)\n",
    "    \n",
    "    order_shopee_extract = order_shopee_tmp.filter((trunc(col('create_time'),'month') < predict_month) & \n",
    "                                                   (trunc(col('create_time'),'month') >= min_month))\n",
    "    \n",
    "    qty_order = order_product_shopee.groupBy('ordersn').agg(sum('original_price').alias('revenue'))\\\n",
    "    .select('ordersn', 'revenue')\n",
    "    \n",
    "    days_diff = order_shopee_extract.select('create_time', 'buyer_username')\n",
    "    days_diff = days_diff.withColumn('date_only', to_date(col('create_time')))\n",
    "    w = Window().partitionBy('buyer_username').orderBy(['buyer_username', 'create_time'])\n",
    "    days_diff = days_diff.dropDuplicates([\"buyer_username\",\"date_only\"])\n",
    "    days_diff = days_diff.select(\"*\", lag(\"date_only\").over(w).alias(\"new_col_1\"))\\\n",
    "    .withColumn('days_diff', datediff(col('date_only'),col(\"new_col_1\"))).fillna(999,subset=['days_diff'])\n",
    "    days_diff = days_diff.groupby(\"buyer_username\").agg(min('days_diff').alias('days_diff'))\n",
    "    \n",
    "    order_shopee_extract = order_shopee_extract.join(qty_order, ['ordersn'])\n",
    "    order_shopee_extract = order_shopee_extract.withColumn('count_order', lit(1))\n",
    "    order_shopee_extract = order_shopee_extract.withColumnRenamed('buyer_username','customer_id')\n",
    "    max_all_tgl = order_shopee_extract.agg(max('create_time')).collect()[0][0]\n",
    "    order_shopee_extract = order_shopee_extract.withColumn('max_all_tgl', lit(max_all_tgl))\n",
    "    \n",
    "    final_data = order_shopee_extract.groupBy('customer_id')\\\n",
    "    .agg(sum('count_order').alias('frequency'),max('recipient_province').alias('dc_code'),sum('revenue').alias('revenue'),\n",
    "         max('create_time').alias('max_tgl'),max('max_all_tgl').alias('max_all_tgl'))\\\n",
    "    .withColumn('recency', datediff(col('max_all_tgl'), col('max_tgl')))\\\n",
    "    .select('customer_id','frequency','revenue','recency','dc_code')\n",
    "    \n",
    "    days_diff = days_diff.withColumnRenamed('buyer_username','customer_id')\n",
    "    final_data = final_data.join(days_diff, ['customer_id'])\n",
    "    \n",
    "    order_shopee_tmp = order_shopee_tmp.withColumn('predict_month', lit(predict_month))\n",
    "    tmp_cond1 = months_between(trunc(order_shopee_tmp.create_time, 'month'),\n",
    "                               trunc(order_shopee_tmp.predict_month, 'month')) == 0\n",
    "    cond_1 = order_shopee_tmp.filter(tmp_cond1).select('buyer_username')\n",
    "    \n",
    "    tmp_cond2 = months_between(trunc(order_shopee_tmp.create_time, 'month'),\n",
    "                                trunc(order_shopee_tmp.predict_month, 'month')) < 0\n",
    "    cond_2 = order_shopee_tmp.join(cond_1, ['buyer_username']).filter(tmp_cond2)\\\n",
    "    .select('buyer_username').distinct().withColumn('label', lit(1))\\\n",
    "    .withColumnRenamed('buyer_username','customer_id')\n",
    "    \n",
    "    final_data = cond_2.join(final_data, ['customer_id'],'right').fillna(value=0)\n",
    "    \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "varied-conversation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_bc():\n",
    "    # Slicing data agar 6 bulan saja\n",
    "    order_bc_tmp = order_bc.select('order_id', 'date_order', 'customer_id')\n",
    "    predict_month = order_bc_tmp.agg(trunc(max(col('date_order')),'month')).collect()[0][0]\n",
    "    min_month = predict_month + relativedelta(months=-6)\n",
    "    order_bc_extract = order_bc_tmp.filter((trunc(col('date_order'),'month') < predict_month) & \n",
    "                                           (trunc(col('date_order'),'month') >= min_month))\n",
    "    \n",
    "    # Ekstrak fitur quantity, days_diff, dc_code, dan count_order\n",
    "    qty_order = order_product_bc.groupBy('order_id').agg(sum('line_subtotal').alias('revenue'))\\\n",
    "    .select('order_id', 'revenue')\n",
    "\n",
    "    dc_user = order_shipping_bc.select('order_id', 'shipping_province')\n",
    "    \n",
    "    days_diff = order_bc_extract.select('date_order', 'customer_id')\n",
    "    days_diff = days_diff.withColumn('date_only', to_date(col('date_order')))\n",
    "    w = Window().partitionBy('customer_id').orderBy(['customer_id', 'date_order'])\n",
    "    days_diff = days_diff.dropDuplicates([\"customer_id\",\"date_only\"])\n",
    "    days_diff = days_diff.select(\"*\", lag(\"date_only\").over(w).alias(\"new_col_1\"))\\\n",
    "    .withColumn('days_diff', datediff(col('date_only'),col(\"new_col_1\"))).fillna(999,subset=['days_diff'])\n",
    "    days_diff = days_diff.groupby(\"customer_id\").agg(min('days_diff').alias('days_diff'))\n",
    "    \n",
    "    order_bc_extract = order_bc_extract.join(qty_order, ['order_id'])\n",
    "    order_bc_extract = order_bc_extract.join(dc_user, ['order_id'])\n",
    "    order_bc_extract = order_bc_extract.withColumn('count_order', lit(1))\n",
    "    max_all_tgl = order_bc_extract.agg(max('date_order')).collect()[0][0]\n",
    "    order_bc_extract = order_bc_extract.withColumn('max_all_tgl', lit(max_all_tgl))\n",
    "    \n",
    "    final_data = order_bc_extract.groupBy('customer_id')\\\n",
    "    .agg(sum('count_order').alias('frequency'),sum('revenue').alias('revenue'), max('shipping_province').alias('dc_code'),\n",
    "         max('date_order').alias('max_tgl'),max('max_all_tgl').alias('max_all_tgl'))\\\n",
    "    .withColumn('recency', datediff(col('max_all_tgl'), col('max_tgl')))\\\n",
    "    .select('customer_id','frequency','revenue','recency','dc_code')\n",
    "    \n",
    "    final_data = final_data.join(days_diff, ['customer_id'])\n",
    "    \n",
    "    # Ekstrak fitur label\n",
    "    order_bc_tmp = order_bc_tmp.withColumn('predict_month', lit(predict_month))\n",
    "    tmp_cond1 = months_between(trunc(order_bc_tmp.date_order, 'month'),\n",
    "                               trunc(order_bc_tmp.predict_month, 'month')) == 0\n",
    "    cond_1 = order_bc_tmp.filter(tmp_cond1).select('customer_id')\n",
    "    \n",
    "    tmp_cond2 = months_between(trunc(order_bc_tmp.date_order, 'month'),\n",
    "                                trunc(order_bc_tmp.predict_month, 'month')) < 0\n",
    "    cond_2 = order_bc_tmp.join(cond_1, ['customer_id']).filter(tmp_cond2)\\\n",
    "    .select('customer_id').distinct().withColumn('label', lit(1))\n",
    "    \n",
    "    final_data = cond_2.join(final_data, ['customer_id'],'right').fillna(value=0)\n",
    "    \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cellular-miller",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_data_bc = transform_data_bc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "developmental-functionality",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_data_shopee = transform_data_shopee()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "homeless-strength",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# final_data = final_data_shopee.union(final_data_bc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "latin-violin",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = final_data_bc.select('customer_id','frequency','revenue','recency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "appointed-singer",
   "metadata": {},
   "outputs": [],
   "source": [
    "va = VectorAssembler(outputCol='features_frequency')\n",
    "va.setInputCols(['frequency'])\n",
    "combine_data = va.transform(a)\n",
    "\n",
    "va = VectorAssembler(outputCol='features_revenue')\n",
    "va.setInputCols(['revenue'])\n",
    "combine_data = va.transform(combine_data)\n",
    "\n",
    "va = VectorAssembler(outputCol='features_recency')\n",
    "va.setInputCols(['recency'])\n",
    "combine_data = va.transform(combine_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bottom-recycling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[customer_id: string, frequency: bigint, revenue: decimal(38,18), recency: int, features_frequency: vector, features_revenue: vector, features_recency: vector]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cathedral-poverty",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "KMeans_algo=KMeans(featuresCol='features_frequency', k=7, predictionCol='cluster_frequency')\n",
    "KMeans_fit=KMeans_algo.fit(combine_data)\n",
    "output=KMeans_fit.transform(combine_data)\n",
    "\n",
    "KMeans_algo=KMeans(featuresCol='features_revenue', k=4, predictionCol='cluster_revenue')\n",
    "KMeans_fit=KMeans_algo.fit(output)\n",
    "output=KMeans_fit.transform(output)\n",
    "\n",
    "KMeans_algo=KMeans(featuresCol='features_recency', k=2, predictionCol='cluster_recency')\n",
    "KMeans_fit=KMeans_algo.fit(output)\n",
    "output=KMeans_fit.transform(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "unnecessary-concrete",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[customer_id: string, frequency: bigint, revenue: decimal(38,18), recency: int, features_frequency: vector, features_revenue: vector, features_recency: vector]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_data.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "graduate-idaho",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = output.drop('features_frequency', 'features_revenue', 'features_recency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "compound-birmingham",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_bc = final_data_bc.join(output,['customer_id','frequency','revenue','recency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "alien-islam",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[customer_id: string, frequency: bigint, revenue: decimal(38,18), recency: int, label: int, dc_code: string, days_diff: int, cluster_frequency: int, cluster_revenue: int, cluster_recency: int]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data_bc.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-passion",
   "metadata": {},
   "source": [
    "#  ML Proses Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "spiritual-framing",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator=MulticlassClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "changing-accused",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_feature(data):\n",
    "    indexer = StringIndexer(inputCol='dc_code', outputCol='dc_code_num')\n",
    "    indexd_data=indexer.fit(data).transform(data)\n",
    "    encoder = OneHotEncoder(inputCol='dc_code_num', outputCol = 'dc_code_vec')\n",
    "    onehotdata = encoder.fit(indexd_data).transform(indexd_data)\n",
    "    encoder = OneHotEncoder(inputCol='cluster_frequency', outputCol = 'cluster_frequency_vec')\n",
    "    onehotdata = encoder.fit(indexd_data).transform(onehotdata)\n",
    "    encoder = OneHotEncoder(inputCol='cluster_revenue', outputCol = 'cluster_revenue_vec')\n",
    "    onehotdata = encoder.fit(indexd_data).transform(onehotdata)\n",
    "    encoder = OneHotEncoder(inputCol='cluster_recency', outputCol = 'cluster_recency_vec')\n",
    "    onehotdata = encoder.fit(indexd_data).transform(onehotdata)\n",
    "    va = VectorAssembler(outputCol='features')\n",
    "    va.setInputCols(['days_diff','dc_code_vec','cluster_frequency_vec','cluster_revenue_vec','cluster_recency_vec','frequency','revenue','recency'])\n",
    "    combine_data = va.transform(onehotdata).select(['features','label'])\n",
    "    return combine_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ethical-nursery",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_oversampling_data(train_data, potentialDf_count, nonPotentialDf_count):\n",
    "    potentialDf = train_data.filter(\"label=1.0\")\n",
    "    nonPotentialDf = train_data.filter(\"label=0.0\")\n",
    "    sampleRatio = nonPotentialDf_count / potentialDf_count\n",
    "    a = range(int(sampleRatio))\n",
    "    oversampled_df = potentialDf.withColumn(\"dummy\", explode(array([lit(x) for x in a]))).drop('dummy')\n",
    "    train_df = nonPotentialDf.unionAll(oversampled_df)\n",
    "    return train_df\n",
    "\n",
    "def make_undersampling_data(train_data, potentialDf_count, nonPotentialDf_count):\n",
    "    potentialDf = train_data.filter(\"label=1.0\")\n",
    "    nonPotentialDf= train_data.filter(\"label=0.0\")\n",
    "    sampleRatio = potentialDf_count / nonPotentialDf_count\n",
    "    nonPotentialSampleDf = nonPotentialDf.sample(False, sampleRatio)\n",
    "    train_df = potentialDf.unionAll(nonPotentialSampleDf)\n",
    "    return train_df\n",
    "\n",
    "def make_weighted_data(train_data, potentialDf_count, nonPotentialDf_count):\n",
    "    ratioOfFraud = potentialDf_count / train_data.count()\n",
    "    fraudWeight  = 1 - ratioOfFraud\n",
    "    nonFraudWeight = ratioOfFraud\n",
    "    weightedDF = train_data.withColumn(\"weight\",when(train_data.label == 1,fraudWeight).otherwise(nonFraudWeight))\n",
    "    return weightedDF\n",
    "\n",
    "# def make_smote_data(vectorized_sdf,smote_config):\n",
    "#     '''\n",
    "#     contains logic to perform smote oversampling, given a spark df with 2 classes\n",
    "#     inputs:\n",
    "#     * vectorized_sdf: cat cols are already stringindexed, num cols are assembled into 'features' vector\n",
    "#       df target col should be 'label'\n",
    "#     * smote_config: config obj containing smote parameters\n",
    "#     output:\n",
    "#     * oversampled_df: spark df after smote oversampling\n",
    "#     '''\n",
    "#     dataInput_min = vectorized_sdf[vectorized_sdf['label'] == 1]\n",
    "#     dataInput_maj = vectorized_sdf[vectorized_sdf['label'] == 0]\n",
    "    \n",
    "#     # LSH, bucketed random projection\n",
    "#     brp = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\",seed=smote_config.seed, bucketLength=smote_config.bucketLength)\n",
    "#     # smote only applies on existing minority instances    \n",
    "#     model = brp.fit(dataInput_min)\n",
    "#     model.transform(dataInput_min)\n",
    "\n",
    "#     # here distance is calculated from brp's param inputCol\n",
    "#     self_join_w_distance = model.approxSimilarityJoin(dataInput_min, dataInput_min, float(\"inf\"), distCol=\"EuclideanDistance\")\n",
    "\n",
    "#     # remove self-comparison (distance 0)\n",
    "#     self_join_w_distance = self_join_w_distance.filter(self_join_w_distance.EuclideanDistance > 0)\n",
    "\n",
    "#     over_original_rows = Window.partitionBy(\"datasetA\").orderBy(\"EuclideanDistance\")\n",
    "\n",
    "#     self_similarity_df = self_join_w_distance.withColumn(\"r_num\", row_number().over(over_original_rows))\n",
    "\n",
    "#     self_similarity_df_selected = self_similarity_df.filter(self_similarity_df.r_num <= smote_config.k)\n",
    "\n",
    "#     over_original_rows_no_order = Window.partitionBy('datasetA')\n",
    "\n",
    "#     # list to store batches of synthetic data\n",
    "#     res = []\n",
    "    \n",
    "#     # two udf for vector add and subtract, subtraction include a random factor [0,1]\n",
    "#     subtract_vector_udf = udf(lambda arr: random.uniform(0, 1)*(arr[0]-arr[1]), VectorUDT())\n",
    "#     add_vector_udf = udf(lambda arr: arr[0]+arr[1], VectorUDT())\n",
    "    \n",
    "#     # retain original columns\n",
    "#     original_cols = dataInput_min.columns\n",
    "    \n",
    "#     for i in range(smote_config.multiplier):\n",
    "#         # logic to randomly select neighbour: pick the largest random number generated row as the neighbour\n",
    "#         df_random_sel = self_similarity_df_selected.withColumn(\"rand\", rand()).withColumn('max_rand', max('rand').over(over_original_rows_no_order))\\\n",
    "#                             .where(col('rand') == col('max_rand')).drop(*['max_rand','rand','r_num'])\n",
    "#         # create synthetic feature numerical part\n",
    "#         df_vec_diff = df_random_sel.select('*', subtract_vector_udf(array('datasetA.features', 'datasetB.features')).alias('vec_diff'))\n",
    "#         df_vec_modified = df_vec_diff.select('*', add_vector_udf(array('datasetA.features', 'vec_diff')).alias('features'))\n",
    "        \n",
    "#         # for categorical cols, either pick original or the neighbour's cat values\n",
    "#         for c in original_cols:\n",
    "#             # randomly select neighbour or original data\n",
    "#             col_sub = random.choice(['datasetA','datasetB'])\n",
    "#             val = \"{0}.{1}\".format(col_sub,c)\n",
    "#             if c != 'features':\n",
    "#                 # do not unpack original numerical features\n",
    "#                 df_vec_modified = df_vec_modified.withColumn(c,col(val))\n",
    "        \n",
    "#         # this df_vec_modified is the synthetic minority instances,\n",
    "#         df_vec_modified = df_vec_modified.drop(*['datasetA','datasetB','vec_diff','EuclideanDistance'])\n",
    "        \n",
    "#         res.append(df_vec_modified)\n",
    "    \n",
    "#     dfunion = reduce(DataFrame.unionAll, res)\n",
    "#     # union synthetic instances with original full (both minority and majority) df\n",
    "#     oversampled_df = dfunion.union(vectorized_sdf.select(dfunion.columns))\n",
    "    \n",
    "#     return oversampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "polish-template",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_norm_1(data):\n",
    "    normalizer = Normalizer(inputCol=\"features\")\n",
    "    data = normalizer.transform(data, {normalizer.p: 1, normalizer.outputCol:\"norm_1\"})\n",
    "    return data.select('label',\"norm_1\")\n",
    "\n",
    "def transform_data_norm_inf(data):\n",
    "    normalizer = Normalizer(inputCol=\"features\")\n",
    "    data = normalizer.transform(data, {normalizer.p: float(\"inf\"), normalizer.outputCol:\"norm_inf\"})\n",
    "    return data.select('label',\"norm_inf\")\n",
    "\n",
    "def transform_data_standard_scaler(data):\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "    scalerModel = scaler.fit(data)\n",
    "    data = scalerModel.transform(data)\n",
    "    return data.select('label',\"scaled_features\")\n",
    "\n",
    "def transform_data_minmax_scaler(data):\n",
    "    mmscaler = MinMaxScaler(inputCol=\"features\", outputCol=\"mmscaled_features\")\n",
    "    mmscalerModel = mmscaler.fit(data)\n",
    "    data = mmscalerModel.transform(data)\n",
    "    return data.select('label',\"mmscaled_features\")\n",
    "\n",
    "def transform_data_maxabs_scaler(data):\n",
    "    mascaler = MaxAbsScaler(inputCol=\"features\", outputCol=\"mascaled_features\")\n",
    "    mascalerModel = mascaler.fit(data)\n",
    "    data = mascalerModel.transform(data)\n",
    "    return data.select('label',\"mascaled_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "potential-mayor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsvc_train_tuning_model(train_data, features):\n",
    "    model = LinearSVC(labelCol=\"label\", maxIter=10, featuresCol=features)\n",
    "    trained_model = model.fit(train_data)\n",
    "    return trained_model\n",
    "\n",
    "def lr_train_tuning_model(train_data, features):\n",
    "    model = LogisticRegression(labelCol=\"label\", maxIter=10, featuresCol=features)\n",
    "    trained_model = model.fit(train_data)\n",
    "    return trained_model\n",
    "\n",
    "def rf_train_tuning_model(train_data, features):\n",
    "    model = RandomForestClassifier(labelCol=\"label\", featuresCol=features)\n",
    "    trained_model = model.fit(train_data)\n",
    "    return trained_model\n",
    "\n",
    "def gbt_train_tuning_model(train_data, features):\n",
    "    model = GBTClassifier(labelCol=\"label\", maxIter=10, featuresCol=features)\n",
    "    trained_model = model.fit(train_data)\n",
    "    return trained_model\n",
    "\n",
    "def dt_train_tuning_model(train_data, features):\n",
    "    model = DecisionTreeClassifier(labelCol=\"label\", featuresCol=features)\n",
    "    trained_model = model.fit(train_data)\n",
    "    return trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "convenient-black",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_model_f1(pred):\n",
    "    f1 = evaluator.evaluate(pred,{evaluator.metricName: \"f1\"})  \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "needed-luxury",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_model_precision(pred):\n",
    "    precision = evaluator.evaluate(pred,{evaluator.metricName: \"precisionByLabel\"}) \n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "operating-grass",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_model_recall(pred):\n",
    "    recall = evaluator.evaluate(pred,{evaluator.metricName: \"recallByLabel\"})\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-designer",
   "metadata": {},
   "source": [
    "## Prepare data for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "welsh-pocket",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combine_data = combine_feature(final_data)\n",
    "raw_data_train, raw_data_test = combine_data.randomSplit([0.7, 0.3], seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dental-stone",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "potential_count = raw_data_train.filter(\"label=1.0\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "excited-elephant",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonPotential_count = raw_data_train.filter(\"label=0.0\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "level-watson",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_over = make_oversampling_data(raw_data_train,potential_count,nonPotential_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dressed-genre",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_under = make_undersampling_data(raw_data_train,potential_count,nonPotential_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "binding-tutorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf = Section(\"smote_config\")\n",
    "# conf.seed = 48\n",
    "# conf.bucketLength = 100\n",
    "# conf.k = 4\n",
    "# conf.multiplier = int(nonPotential_count/potential_count)+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "graphic-hearts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_train_smote = make_smote_data(raw_data_train,conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "structural-dimension",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_weighted = make_weighted_data(raw_data_train,potential_count,nonPotential_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "nuclear-screen",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_original_train = data_train_under\n",
    "data_original_test = raw_data_test\n",
    "\n",
    "data_norm_1_train = transform_data_norm_1(data_train_under)\n",
    "data_norm_1_test = transform_data_norm_1(raw_data_test)\n",
    "\n",
    "data_norm_inf_train = transform_data_norm_inf(data_train_under)\n",
    "data_norm_inf_test = transform_data_norm_inf(raw_data_test)\n",
    "\n",
    "data_standard_scaler_train = transform_data_standard_scaler(data_train_under)\n",
    "data_standard_scaler_test = transform_data_standard_scaler(raw_data_test)\n",
    "\n",
    "data_minmax_scaler_train = transform_data_minmax_scaler(data_train_under)\n",
    "data_minmax_scaler_test = transform_data_minmax_scaler(raw_data_test)\n",
    "\n",
    "data_maxabs_scaler_train = transform_data_maxabs_scaler(data_train_under)\n",
    "data_maxabs_scaler_test = transform_data_maxabs_scaler(raw_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "banner-tension",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[customer_id: string, frequency: bigint, revenue: decimal(38,18), recency: int, label: int, dc_code: string, days_diff: int, cluster_frequency: int, cluster_revenue: int, cluster_recency: int]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-symphony",
   "metadata": {},
   "source": [
    "## ML proses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "german-projector",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_model = {}\n",
    "metric = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-proposition",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "romantic-graduation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector, label: int]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_original_train.cache()\n",
    "data_original_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "crude-debate",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "lsvc_tuning_original = lsvc_train_tuning_model(data_original_train,'features')\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "printable-local",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lsvc_tuning_original.transform(data_original_test)\n",
    "f1 = metric_model_f1(pred)\n",
    "precision = metric_model_precision(pred)\n",
    "recall = metric_model_recall(pred)\n",
    "dict_model['svm_original'] = lsvc_tuning_original\n",
    "metric.append(('svm', 'original', (end-start), f1, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "minimal-treatment",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector, label: int]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_original_train.unpersist()\n",
    "data_original_test.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "collective-cameroon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, norm_inf: vector]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_norm_inf_train.cache()\n",
    "data_norm_inf_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "comparable-abraham",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "lsvc_tuning_norm_inf = lsvc_train_tuning_model(data_norm_inf_train,'norm_inf')\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "changed-postage",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lsvc_tuning_norm_inf.transform(data_norm_inf_test)\n",
    "f1 = metric_model_f1(pred)\n",
    "precision = metric_model_precision(pred)\n",
    "recall = metric_model_recall(pred)\n",
    "dict_model['svm_norm_inf'] = lsvc_tuning_norm_inf\n",
    "metric.append(('svm', 'norm_inf', (end-start), f1, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "impressive-amsterdam",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, norm_inf: vector]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_norm_inf_train.unpersist()\n",
    "data_norm_inf_test.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "mysterious-cradle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, norm_1: vector]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_norm_1_train.cache()\n",
    "data_norm_1_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "corrected-booking",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "lsvc_tuning_norm_1 = lsvc_train_tuning_model(data_norm_1_train,'norm_1')\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "unlimited-terrorism",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lsvc_tuning_norm_1.transform(data_norm_1_test)\n",
    "f1 = metric_model_f1(pred)\n",
    "precision = metric_model_precision(pred)\n",
    "recall = metric_model_recall(pred)\n",
    "dict_model['svm_norm_1'] = lsvc_tuning_norm_1\n",
    "metric.append(('svm', 'norm_1', (end-start), f1, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "temporal-valuable",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, norm_1: vector]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_norm_1_train.unpersist()\n",
    "data_norm_1_test.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "discrete-arrow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, scaled_features: vector]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_standard_scaler_train.cache()\n",
    "data_standard_scaler_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "global-decline",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "lsvc_tuning_scaled_features = lsvc_train_tuning_model(data_standard_scaler_train,'scaled_features')\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "consolidated-rally",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lsvc_tuning_scaled_features.transform(data_standard_scaler_test)\n",
    "f1 = metric_model_f1(pred)\n",
    "precision = metric_model_precision(pred)\n",
    "recall = metric_model_recall(pred)\n",
    "dict_model['svm_scaled_features'] = lsvc_tuning_scaled_features\n",
    "metric.append(('svm', 'scaled_features', (end-start), f1, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "spectacular-madrid",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, scaled_features: vector]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_standard_scaler_train.unpersist()\n",
    "data_standard_scaler_test.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "sought-weather",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, mmscaled_features: vector]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_minmax_scaler_train.cache()\n",
    "data_minmax_scaler_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "about-topic",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "lsvc_tuning_mmscaled_features = lsvc_train_tuning_model(data_minmax_scaler_train,'mmscaled_features')\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "portuguese-jackson",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lsvc_tuning_mmscaled_features.transform(data_minmax_scaler_test)\n",
    "f1 = metric_model_f1(pred)\n",
    "precision = metric_model_precision(pred)\n",
    "recall = metric_model_recall(pred)\n",
    "dict_model['svm_mmscaled_features'] = lsvc_tuning_mmscaled_features\n",
    "metric.append(('svm', 'mmscaled_features', (end-start), f1, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "advisory-beauty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, mmscaled_features: vector]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_minmax_scaler_train.unpersist()\n",
    "data_minmax_scaler_test.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "technical-entrepreneur",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, mascaled_features: vector]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_maxabs_scaler_train.cache()\n",
    "data_maxabs_scaler_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "female-municipality",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "lsvc_tuning_mascaled_features = lsvc_train_tuning_model(data_maxabs_scaler_train,'mascaled_features')\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "enabling-conducting",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lsvc_tuning_mascaled_features.transform(data_maxabs_scaler_test)\n",
    "f1 = metric_model_f1(pred)\n",
    "precision = metric_model_precision(pred)\n",
    "recall = metric_model_recall(pred)\n",
    "dict_model['svm_mascaled_features'] = lsvc_tuning_mascaled_features\n",
    "metric.append(('svm', 'mascaled_features', (end-start), f1, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "noble-bunch",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, mascaled_features: vector]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_maxabs_scaler_train.unpersist()\n",
    "data_maxabs_scaler_test.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-proportion",
   "metadata": {},
   "source": [
    "# LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "governmental-strategy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector, label: int]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_original_train.cache()\n",
    "data_original_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "empty-devon",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "lr_tuning_original = lr_train_tuning_model(data_original_train,'features')\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "comparable-gender",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr_tuning_original.transform(data_original_test)\n",
    "f1 = metric_model_f1(pred)\n",
    "precision = metric_model_precision(pred)\n",
    "recall = metric_model_recall(pred)\n",
    "dict_model['lr_original'] = lr_tuning_original\n",
    "metric.append(('lr', 'original', (end-start), f1, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "stuck-album",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector, label: int]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_original_train.unpersist()\n",
    "data_original_test.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "minor-photographer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, norm_inf: vector]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_norm_inf_train.cache()\n",
    "data_norm_inf_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "productive-syndicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "lr_tuning_norm_inf = lr_train_tuning_model(data_norm_inf_train,'norm_inf')\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "distant-stake",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr_tuning_norm_inf.transform(data_norm_inf_test)\n",
    "f1 = metric_model_f1(pred)\n",
    "precision = metric_model_precision(pred)\n",
    "recall = metric_model_recall(pred)\n",
    "dict_model['lr_norm_inf'] = lr_tuning_norm_inf\n",
    "metric.append(('lr', 'norm_inf', (end-start), f1, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "parental-statistics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, norm_inf: vector]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_norm_inf_train.unpersist()\n",
    "data_norm_inf_test.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "other-yorkshire",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, norm_1: vector]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_norm_1_train.cache()\n",
    "data_norm_1_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "unavailable-alexander",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "lr_tuning_norm_1 = lr_train_tuning_model(data_norm_1_train,'norm_1')\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "extended-disorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr_tuning_norm_1.transform(data_norm_1_test)\n",
    "f1 = metric_model_f1(pred)\n",
    "precision = metric_model_precision(pred)\n",
    "recall = metric_model_recall(pred)\n",
    "dict_model['lr_norm_1'] = lr_tuning_norm_1\n",
    "metric.append(('lr', 'norm_1', (end-start), f1, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "concrete-monday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, norm_1: vector]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_norm_1_train.unpersist()\n",
    "data_norm_1_test.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "conscious-array",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, scaled_features: vector]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_standard_scaler_train.cache()\n",
    "data_standard_scaler_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "offensive-motivation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "lr_tuning_scaled_features = lr_train_tuning_model(data_standard_scaler_train,'scaled_features')\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "present-scout",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr_tuning_scaled_features.transform(data_standard_scaler_test)\n",
    "f1 = metric_model_f1(pred)\n",
    "precision = metric_model_precision(pred)\n",
    "recall = metric_model_recall(pred)\n",
    "dict_model['lr_scaled_features'] = lr_tuning_scaled_features\n",
    "metric.append(('lr', 'scaled_features', (end-start), f1, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "latter-longer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, scaled_features: vector]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_standard_scaler_train.unpersist()\n",
    "data_standard_scaler_test.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "limited-accommodation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, mmscaled_features: vector]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_minmax_scaler_train.cache()\n",
    "data_minmax_scaler_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "frank-moore",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "lr_tuning_mmscaled_features = lr_train_tuning_model(data_minmax_scaler_train,'mmscaled_features')\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "absent-syndication",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr_tuning_mmscaled_features.transform(data_minmax_scaler_test)\n",
    "f1 = metric_model_f1(pred)\n",
    "precision = metric_model_precision(pred)\n",
    "recall = metric_model_recall(pred)\n",
    "dict_model['lr_mmscaled_features'] = lr_tuning_mmscaled_features\n",
    "metric.append(('lr', 'mmscaled_features', (end-start), f1, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "interior-league",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, mmscaled_features: vector]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_minmax_scaler_train.unpersist()\n",
    "data_minmax_scaler_test.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "third-october",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, mascaled_features: vector]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_maxabs_scaler_train.cache()\n",
    "data_maxabs_scaler_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ahead-beads",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "lr_tuning_mascaled_features = lr_train_tuning_model(data_maxabs_scaler_train,'mascaled_features')\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "hydraulic-astrology",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr_tuning_mascaled_features.transform(data_maxabs_scaler_test)\n",
    "f1 = metric_model_f1(pred)\n",
    "precision = metric_model_precision(pred)\n",
    "recall = metric_model_recall(pred)\n",
    "dict_model['lr_mascaled_features'] = lr_tuning_mascaled_features\n",
    "metric.append(('lr', 'mascaled_features', (end-start), f1, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "solved-colombia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, mascaled_features: vector]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_maxabs_scaler_train.unpersist()\n",
    "data_maxabs_scaler_test.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-platform",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "geological-broad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector, label: int]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_original_train.cache()\n",
    "data_original_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "otherwise-ownership",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "rf_tuning_original = rf_train_tuning_model(data_original_train,'features')\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "israeli-enclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = rf_tuning_original.transform(data_original_test)\n",
    "f1 = metric_model_f1(pred)\n",
    "precision = metric_model_precision(pred)\n",
    "recall = metric_model_recall(pred)\n",
    "dict_model['rf_original'] = rf_tuning_original\n",
    "metric.append(('rf', 'original', (end-start), f1, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "varying-failure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector, label: int]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_original_train.unpersist()\n",
    "data_original_test.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-chancellor",
   "metadata": {},
   "source": [
    "# GBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "dimensional-joyce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector, label: int]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_original_train.cache()\n",
    "data_original_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "greenhouse-quest",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "gbt_tuning_original = gbt_train_tuning_model(data_original_train,'features')\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "accurate-storm",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = gbt_tuning_original.transform(data_original_test)\n",
    "f1 = metric_model_f1(pred)\n",
    "precision = metric_model_precision(pred)\n",
    "recall = metric_model_recall(pred)\n",
    "dict_model['gbt_original'] = gbt_tuning_original\n",
    "metric.append(('gbt', 'original', (end-start), f1, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "earned-thriller",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector, label: int]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_original_train.unpersist()\n",
    "data_original_test.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-mouth",
   "metadata": {},
   "source": [
    "# DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "veterinary-gamma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector, label: int]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_original_train.cache()\n",
    "data_original_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "canadian-parcel",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "dt_tuning_original = dt_train_tuning_model(data_original_train,'features')\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "viral-roommate",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = dt_tuning_original.transform(data_original_test)\n",
    "f1 = metric_model_f1(pred)\n",
    "precision = metric_model_precision(pred)\n",
    "recall = metric_model_recall(pred)\n",
    "dict_model['dt_original'] = dt_tuning_original\n",
    "metric.append(('dt', 'original', (end-start), f1, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "relative-shopper",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector, label: int]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_original_train.unpersist()\n",
    "data_original_test.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-needle",
   "metadata": {},
   "source": [
    "# Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "agricultural-estate",
   "metadata": {},
   "outputs": [],
   "source": [
    "metricColumns = [\"name_model\",\"normalize\",\"time\",\"f1\",\"precision\",\"recall\"]\n",
    "metricDF = spark.createDataFrame(data=metric, schema = metricColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "varied-account",
   "metadata": {},
   "outputs": [],
   "source": [
    "metricDF = metricDF.sort(col('f1').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "aware-williams",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+------------------+------------------+------------------+------------------+\n",
      "|name_model|        normalize|              time|                f1|         precision|            recall|\n",
      "+----------+-----------------+------------------+------------------+------------------+------------------+\n",
      "|        dt|         original| 97.07473826408386|0.8838003982496018|0.9707267697294679|0.8645471539499591|\n",
      "|       svm|         norm_inf|212.88556623458862|0.8828142814292679|0.9700229493840311|0.8637701037541756|\n",
      "|        rf|         original|493.16929507255554|0.8765892339472175|0.9708959176724301| 0.851550228967104|\n",
      "|       svm|         original| 243.4401512145996|0.8749432962478189|0.9708740628262313|0.8486798442581245|\n",
      "|       svm|mmscaled_features|241.74373984336853|0.8748924665205783|0.9708712062490706|0.8485941198593013|\n",
      "|       svm|mascaled_features|209.46183705329895|0.8748924665205783|0.9708712062490706|0.8485941198593013|\n",
      "|       svm|  scaled_features| 210.2630922794342|0.8748924665205783|0.9708712062490706|0.8485941198593013|\n",
      "|       svm|           norm_1|334.12591218948364|  0.85617924809061|0.9708527703027232|0.8160796849766608|\n",
      "|        lr|         original|230.17523407936096|0.8372611298987014|0.9720888705744996|0.7828075570205517|\n",
      "|       gbt|         original|243.15139842033386|0.8344481707911034|0.9724514668003527|0.7777885317346194|\n",
      "|        lr|         norm_inf| 229.1381709575653|0.8332064898534485|0.9717127393031658| 0.776469482114019|\n",
      "|        lr|mmscaled_features| 84.17735862731934|0.8310039691568774|0.9722710578077948|0.7722938743003783|\n",
      "|        lr|           norm_1|280.89517307281494|0.8127897396895014|0.9723081935168594|0.7427742627701701|\n",
      "|        lr|  scaled_features|200.38261008262634|0.7984356365787654|0.9729928808590829|0.7196010220560582|\n",
      "|        lr|mascaled_features|236.60851621627808|0.7934955295402392|0.9732226242735663|0.7117752140344668|\n",
      "+----------+-----------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metricDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-numbers",
   "metadata": {},
   "source": [
    "# Extract best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "acting-strategy",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_best = metricDF.select('name_model').collect()[0][0]\n",
    "normalize_best = metricDF.select('normalize').collect()[0][0]\n",
    "name_best_model = name_best + '_' + normalize_best\n",
    "best_model = dict_model[name_best_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-philip",
   "metadata": {},
   "source": [
    "# Tuning Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "stuffed-newcastle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "gross-helping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "limited-composer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "liked-dover",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_original_train = data_train_over\n",
    "# data_original_test = raw_data_test\n",
    "\n",
    "# data_norm_1_train = transform_data_norm_1(data_train_over)\n",
    "# data_norm_1_test = transform_data_norm_1(raw_data_test)\n",
    "\n",
    "# data_norm_inf_train = transform_data_norm_inf(data_train_over)\n",
    "# data_norm_inf_test = transform_data_norm_inf(raw_data_test)\n",
    "\n",
    "# data_standard_scaler_train = transform_data_standard_scaler(data_train_over)\n",
    "# data_standard_scaler_test = transform_data_standard_scaler(raw_data_test)\n",
    "\n",
    "# data_minmax_scaler_train = transform_data_minmax_scaler(data_train_over)\n",
    "# data_minmax_scaler_test = transform_data_minmax_scaler(raw_data_test)\n",
    "\n",
    "# data_maxabs_scaler_train = transform_data_maxabs_scaler(data_train_over)\n",
    "# data_maxabs_scaler_test = transform_data_maxabs_scaler(raw_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "still-malpractice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if normalize_best == 'original':\n",
    "#     svm_tuning(data, features)\n",
    "# if normalize_best == 'scaled_features':\n",
    "#     lr_tuning(data, features)\n",
    "# if normalize_best == 'mmscaled_features':\n",
    "#     dt_tuning(data, features)\n",
    "# if normalize_best == 'norm_inf':\n",
    "#     rf_tuning(data, features)\n",
    "# if normalize_best == 'mascaled_features':\n",
    "#     gbt_tuning(data, features)\n",
    "# if normalize_best == 'norm_1':\n",
    "#     gbt_tuning(data, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "organizational-message",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if name_best == 'svm':\n",
    "#     svm_tuning(data, features)\n",
    "# if name_best == 'lr':\n",
    "#     lr_tuning(data, features)\n",
    "# if name_best == 'dt':\n",
    "#     dt_tuning(data, features)\n",
    "# if name_best == 'rf':\n",
    "#     rf_tuning(data, features)\n",
    "# if name_best == 'gbt':\n",
    "#     gbt_tuning(data, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "specialized-mouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def svm_tuning(data, features):\n",
    "#     model = LinearSVC(labelCol=\"label\", featuresCol=features, maxIter=10)\n",
    "#     param_grid = ParamGridBuilder() \\\n",
    "#     .addGrid(model.regParam, [0, 0.1, 0.01])\\\n",
    "#     .build()\n",
    "#     crossval = CrossValidator(estimator=model, \n",
    "#                           estimatorParamMaps=param_grid,\n",
    "#                           evaluator=evaluator,\n",
    "#                           numFolds=3)\n",
    "#     trained_model = crossval.fit(train_data)\n",
    "#     return trained_model\n",
    "\n",
    "# def lr_tuning(data, features):\n",
    "#     model = LogisticRegression(labelCol=\"label\", featuresCol=features, maxIter=10)\n",
    "#     param_grid = ParamGridBuilder() \\\n",
    "#     .addGrid(model.regParam, [0, 0.1, 0.01])\\\n",
    "#     .addGrid(model.elasticNetParam, [0, 0.8, 1])\\\n",
    "#     .build()\n",
    "#     crossval = CrossValidator(estimator=model, \n",
    "#                           estimatorParamMaps=param_grid,\n",
    "#                           evaluator=evaluator,\n",
    "#                           numFolds=3)\n",
    "#     trained_model = crossval.fit(train_data)\n",
    "#     return trained_model\n",
    "            \n",
    "# def rf_tuning(data, features):\n",
    "#     model = RandomForestClassifier(labelCol=\"label\", featuresCol=features)\n",
    "#     param_grid = ParamGridBuilder() \\\n",
    "#     .addGrid(model.numTrees, [5, 15, 25])\\\n",
    "#     .addGrid(model.maxDepth, [5, 15, 25])\\\n",
    "#     .addGrid(model.maxBins, [10, 40, 100])\\\n",
    "#     .build()\n",
    "#     crossval = CrossValidator(estimator=model, \n",
    "#                           estimatorParamMaps=param_grid,\n",
    "#                           evaluator=evaluator,\n",
    "#                           numFolds=3)\n",
    "#     trained_model = crossval.fit(train_data)\n",
    "#     return trained_model\n",
    "      \n",
    "# def gbt_tuning(data, features):\n",
    "#     model = GBTClassifier(labelCol=\"label\", featuresCol=features)\n",
    "#     param_grid = ParamGridBuilder() \\\n",
    "#     .addGrid(model.maxDepth, [5, 15, 25])\\\n",
    "#     .addGrid(model.maxBins, [10, 40, 100])\\\n",
    "#     .addGrid(model.maxIter, [5, 10, 20])\\\n",
    "#     .build()\n",
    "#     crossval = CrossValidator(estimator=model, \n",
    "#                           estimatorParamMaps=param_grid,\n",
    "#                           evaluator=evaluator,\n",
    "#                           numFolds=3)\n",
    "#     trained_model = crossval.fit(train_data)\n",
    "#     return trained_model\n",
    "        \n",
    "# def dt_tuning(data, features):\n",
    "#     model = DecisionTreeClassifier(labelCol=\"label\", featuresCol=features)\n",
    "#     param_grid = ParamGridBuilder() \\\n",
    "#     .addGrid(model.maxDepth, [5, 15, 25])\\\n",
    "#     .addGrid(model.maxBins, [10, 40, 100])\\\n",
    "#     .build()\n",
    "#     crossval = CrossValidator(estimator=model, \n",
    "#                           estimatorParamMaps=param_grid,\n",
    "#                           evaluator=evaluator,\n",
    "#                           numFolds=3)\n",
    "#     trained_model = crossval.fit(train_data)\n",
    "#     return trained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-elder",
   "metadata": {},
   "source": [
    "# Doing predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "afraid-toilet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_bc_predict():\n",
    "    # Slicing data agar 6 bulan saja\n",
    "    order_bc_tmp = order_bc.select('order_id', 'date_order', 'customer_id')\n",
    "    predict_month = order_bc_tmp.agg(trunc(max(col('date_order')),'month')).collect()[0][0]\n",
    "    min_month = predict_month + relativedelta(months=-5)\n",
    "    order_bc_extract = order_bc_tmp.filter((trunc(col('date_order'),'month') < predict_month) & \n",
    "                                           (trunc(col('date_order'),'month') >= min_month))\n",
    "    \n",
    "    # Ekstrak fitur quantity, days_diff, dc_code, dan count_order\n",
    "    qty_order = order_product_bc.groupBy('order_id').agg(sum('line_subtotal').alias('revenue'))\\\n",
    "    .select('order_id', 'revenue')\n",
    "\n",
    "    dc_user = order_shipping_bc.select('order_id', 'shipping_province')\n",
    "    \n",
    "    days_diff = order_bc_extract.select('date_order', 'customer_id')\n",
    "    days_diff = days_diff.withColumn('date_only', to_date(col('date_order')))\n",
    "    w = Window().partitionBy('customer_id').orderBy(['customer_id', 'date_order'])\n",
    "    days_diff = days_diff.dropDuplicates([\"customer_id\",\"date_only\"])\n",
    "    days_diff = days_diff.select(\"*\", lag(\"date_only\").over(w).alias(\"new_col_1\"))\\\n",
    "    .withColumn('days_diff', datediff(col('date_only'),col(\"new_col_1\"))).fillna(999,subset=['days_diff'])\n",
    "    days_diff = days_diff.groupby(\"customer_id\").agg(min('days_diff').alias('days_diff'))\n",
    "    \n",
    "    order_bc_extract = order_bc_extract.join(qty_order, ['order_id'])\n",
    "    order_bc_extract = order_bc_extract.join(dc_user, ['order_id'])\n",
    "    order_bc_extract = order_bc_extract.withColumn('count_order', lit(1))\n",
    "    max_all_tgl = order_bc_extract.agg(max('date_order')).collect()[0][0]\n",
    "    order_bc_extract = order_bc_extract.withColumn('max_all_tgl', lit(max_all_tgl))\n",
    "    \n",
    "    final_data = order_bc_extract.groupBy('customer_id')\\\n",
    "    .agg(sum('count_order').alias('frequency'),sum('revenue').alias('revenue'), max('shipping_province').alias('dc_code'),\n",
    "         max('date_order').alias('max_tgl'),max('max_all_tgl').alias('max_all_tgl'))\\\n",
    "    .withColumn('recency', datediff(col('max_all_tgl'), col('max_tgl')))\\\n",
    "    .select('customer_id','frequency','revenue','recency','dc_code')\n",
    "    \n",
    "    final_data = final_data.join(days_diff, ['customer_id'])\n",
    "    \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "numerous-sixth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_shopee_predict():\n",
    "    order_shopee_tmp = order_shopee.select('ordersn', 'create_time', 'buyer_username', 'recipient_province')\n",
    "    \n",
    "    predict_month = order_shopee_tmp.agg(trunc(max(col('create_time')),'month')).collect()[0][0]\n",
    "    min_month = predict_month + relativedelta(months=-5)\n",
    "    \n",
    "    order_shopee_extract = order_shopee_tmp.filter((trunc(col('create_time'),'month') < predict_month) & \n",
    "                                                   (trunc(col('create_time'),'month') >= min_month))\n",
    "    \n",
    "    qty_order = order_product_shopee.groupBy('ordersn').agg(sum('original_price').alias('revenue'))\\\n",
    "    .select('ordersn', 'revenue')\n",
    "    \n",
    "    days_diff = order_shopee_extract.select('create_time', 'buyer_username')\n",
    "    days_diff = days_diff.withColumn('date_only', to_date(col('create_time')))\n",
    "    w = Window().partitionBy('buyer_username').orderBy(['buyer_username', 'create_time'])\n",
    "    days_diff = days_diff.dropDuplicates([\"buyer_username\",\"date_only\"])\n",
    "    days_diff = days_diff.select(\"*\", lag(\"date_only\").over(w).alias(\"new_col_1\"))\\\n",
    "    .withColumn('days_diff', datediff(col('date_only'),col(\"new_col_1\"))).fillna(999,subset=['days_diff'])\n",
    "    days_diff = days_diff.groupby(\"buyer_username\").agg(min('days_diff').alias('days_diff'))\n",
    "    \n",
    "    order_shopee_extract = order_shopee_extract.join(qty_order, ['ordersn'])\n",
    "    order_shopee_extract = order_shopee_extract.withColumn('count_order', lit(1))\n",
    "    order_shopee_extract = order_shopee_extract.withColumnRenamed('buyer_username','customer_id')\n",
    "    max_all_tgl = order_shopee_extract.agg(max('create_time')).collect()[0][0]\n",
    "    order_shopee_extract = order_shopee_extract.withColumn('max_all_tgl', lit(max_all_tgl))\n",
    "    \n",
    "    final_data = order_shopee_extract.groupBy('customer_id')\\\n",
    "    .agg(sum('count_order').alias('frequency'),max('recipient_province').alias('dc_code'),sum('revenue').alias('revenue'),\n",
    "         max('create_time').alias('max_tgl'),max('max_all_tgl').alias('max_all_tgl'))\\\n",
    "    .withColumn('recency', datediff(col('max_all_tgl'), col('max_tgl')))\\\n",
    "    .select('customer_id','frequency','revenue','recency','dc_code')\n",
    "    \n",
    "    days_diff = days_diff.withColumnRenamed('buyer_username','customer_id')\n",
    "    final_data = final_data.join(days_diff, ['customer_id'])\n",
    "    \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "weird-aside",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_feature_predict(data):\n",
    "    indexer = StringIndexer(inputCol='dc_code', outputCol='dc_code_num')\n",
    "    indexd_data=indexer.fit(data).transform(data)\n",
    "    encoder = OneHotEncoder(inputCol='dc_code_num', outputCol = 'dc_code_vec')\n",
    "    onehotdata = encoder.fit(indexd_data).transform(indexd_data)\n",
    "    encoder = OneHotEncoder(inputCol='cluster_frequency', outputCol = 'cluster_frequency_vec')\n",
    "    onehotdata = encoder.fit(indexd_data).transform(onehotdata)\n",
    "    encoder = OneHotEncoder(inputCol='cluster_revenue', outputCol = 'cluster_revenue_vec')\n",
    "    onehotdata = encoder.fit(indexd_data).transform(onehotdata)\n",
    "    va = VectorAssembler(outputCol='features')\n",
    "    encoder = OneHotEncoder(inputCol='cluster_recency', outputCol = 'cluster_recency_vec')\n",
    "    onehotdata = encoder.fit(indexd_data).transform(onehotdata)\n",
    "    va = VectorAssembler(outputCol='features')\n",
    "    va.setInputCols(['frequency','revenue','recency','days_diff','dc_code_vec','cluster_frequency_vec','cluster_revenue_vec','cluster_recency_vec'])\n",
    "    combine_data = va.transform(onehotdata).select(['customer_id', 'features'])\n",
    "    return combine_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "great-administration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_predict_data_norm_1(data):\n",
    "    normalizer = Normalizer(inputCol=\"features\")\n",
    "    data = normalizer.transform(data, {normalizer.p: 1, normalizer.outputCol:\"norm_1\"})\n",
    "    return data.select('customer_id',\"norm_1\")\n",
    "\n",
    "def transform_predict_data_norm_inf(data):\n",
    "    normalizer = Normalizer(inputCol=\"features\")\n",
    "    data = normalizer.transform(data, {normalizer.p: float(\"inf\"), normalizer.outputCol:\"norm_inf\"})\n",
    "    return data.select('customer_id',\"norm_inf\")\n",
    "\n",
    "def transform_predict_data_standard_scaler(data):\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "    scalerModel = scaler.fit(data)\n",
    "    data = scalerModel.transform(data)\n",
    "    return data.select('customer_id',\"scaled_features\")\n",
    "\n",
    "def transform_predict_data_minmax_scaler(data):\n",
    "    mmscaler = MinMaxScaler(inputCol=\"features\", outputCol=\"mmscaled_features\")\n",
    "    mmscalerModel = mmscaler.fit(data)\n",
    "    data = mmscalerModel.transform(data)\n",
    "    return data.select('customer_id',\"mmscaled_features\")\n",
    "\n",
    "def transform_predict_data_maxabs_scaler(data):\n",
    "    mascaler = MaxAbsScaler(inputCol=\"features\", outputCol=\"mascaled_features\")\n",
    "    mascalerModel = mascaler.fit(data)\n",
    "    data = mascalerModel.transform(data)\n",
    "    return data.select('customer_id',\"mascaled_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "proof-survey",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_bc = transform_data_bc_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "opening-filling",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_shopee = transform_data_shopee_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "social-graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_total_data = predict_bc.union(predict_shopee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "valid-kruger",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = predict_total_data.select('customer_id','frequency','revenue','recency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "declared-justice",
   "metadata": {},
   "outputs": [],
   "source": [
    "va = VectorAssembler(outputCol='features_frequency')\n",
    "va.setInputCols(['frequency'])\n",
    "combine_data = va.transform(a)\n",
    "\n",
    "va = VectorAssembler(outputCol='features_revenue')\n",
    "va.setInputCols(['revenue'])\n",
    "combine_data = va.transform(combine_data)\n",
    "\n",
    "va = VectorAssembler(outputCol='features_recency')\n",
    "va.setInputCols(['recency'])\n",
    "combine_data = va.transform(combine_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "adult-declaration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[customer_id: string, frequency: bigint, revenue: decimal(38,18), recency: int, features_frequency: vector, features_revenue: vector, features_recency: vector]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "secure-strengthening",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "KMeans_algo=KMeans(featuresCol='features_frequency', k=3, predictionCol='cluster_frequency')\n",
    "KMeans_fit=KMeans_algo.fit(combine_data)\n",
    "output=KMeans_fit.transform(combine_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "sophisticated-receipt",
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeans_algo=KMeans(featuresCol='features_revenue', k=3, predictionCol='cluster_revenue')\n",
    "KMeans_fit=KMeans_algo.fit(output)\n",
    "output=KMeans_fit.transform(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "celtic-subcommittee",
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeans_algo=KMeans(featuresCol='features_recency', k=3, predictionCol='cluster_recency')\n",
    "KMeans_fit=KMeans_algo.fit(output)\n",
    "output=KMeans_fit.transform(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "immune-cartoon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[customer_id: string, frequency: bigint, revenue: decimal(38,18), recency: int, features_frequency: vector, features_revenue: vector, features_recency: vector]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_data.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "raising-liberty",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.drop('features_frequency', 'features_revenue', 'features_recency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "improved-poland",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_total_data = predict_total_data.join(output,['customer_id','frequency','revenue','recency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "secure-poster",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_total_data = combine_feature_predict(predict_total_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "bearing-confidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "if normalize_best == 'original':\n",
    "    predict_total_data = predict_total_data\n",
    "elif normalize_best == 'norm_1':\n",
    "    predict_total_data = transform_predict_data_norm_1(predict_total_data)\n",
    "elif normalize_best == 'norm_inf':\n",
    "    predict_total_data = transform_predict_data_norm_inf(predict_total_data)\n",
    "elif normalize_best == 'standard_scaler':\n",
    "    predict_total_data = transform_predict_data_standard_scaler(predict_total_data)\n",
    "elif normalize_best == 'minmax_scaler':\n",
    "    predict_total_data = transform_predict_data_minmax_scaler(predict_total_data)\n",
    "elif normalize_best == 'maxabs_scaler':\n",
    "    predict_total_data = transform_predict_data_maxabs_scaler(predict_total_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "fitted-foster",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = best_model.transform(predict_total_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-shoulder",
   "metadata": {},
   "source": [
    "# Extract feature for Pattern Mining for Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "complimentary-davis",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_potential_pred = prediction.filter(\"prediction == 1\").select('customer_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "former-township",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_bc_2():\n",
    "    order_bc_tmp = order_bc.select('order_id', 'date_order', 'customer_id')\n",
    "    \n",
    "    predict_month = order_bc_tmp.agg(trunc(max(col('date_order')),'month')).collect()[0][0]\n",
    "    \n",
    "    min_month = predict_month + relativedelta(months=-5)\n",
    "    \n",
    "    order_bc_extract = order_bc_tmp.filter((trunc(col('date_order'),'month') <= predict_month) & \n",
    "                                           (trunc(col('date_order'),'month') >= min_month))\n",
    "    \n",
    "    items_purchase = order_product_bc.groupBy('order_id').agg(collect_set('product_name').alias('items'))\\\n",
    "    .select('order_id', 'items')\n",
    "    \n",
    "    order_bc_extract = order_bc_extract.join(items_purchase, ['order_id'])\n",
    "\n",
    "    final_data = order_bc_extract.withColumn(\"explode\", explode(col(\"items\")))\n",
    "    final_data = final_data.groupBy('customer_id')\\\n",
    "    .agg(collect_set('explode').alias('lst_items'))\\\n",
    "    .select('customer_id','lst_items')\n",
    "    \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "prescribed-process",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_shopee_2():\n",
    "    order_shopee_tmp = order_shopee.select('ordersn', 'create_time', 'buyer_username')\n",
    "    \n",
    "    predict_month = order_shopee_tmp.agg(trunc(max(col('create_time')),'month')).collect()[0][0]\n",
    "    \n",
    "    min_month = predict_month + relativedelta(months=-5)\n",
    "    \n",
    "    order_shopee_extract = order_shopee_tmp.filter((trunc(col('create_time'),'month') <= predict_month) & \n",
    "                                                   (trunc(col('create_time'),'month') >= min_month))\n",
    "    \n",
    "    items_purchase = order_product_shopee.groupBy('ordersn').agg(collect_set('item_name').alias('items'))\\\n",
    "    .select('ordersn', 'items')\n",
    "    \n",
    "    order_shopee_extract = order_shopee_extract.join(items_purchase, ['ordersn'])\n",
    "    order_shopee_extract = order_shopee_extract.withColumnRenamed('buyer_username','customer_id')\n",
    "    \n",
    "    final_data = order_shopee_extract.withColumn(\"explode\", explode(col(\"items\")))\n",
    "    final_data = final_data.groupBy('customer_id')\\\n",
    "    .agg(collect_set('explode').alias('lst_items'))\\\n",
    "    .select('customer_id','lst_items')\n",
    "    \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "apart-burden",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_item_bc = transform_data_bc_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "parallel-specific",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_shopee_bc = transform_data_shopee_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "animated-inside",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_item_total = data_item_bc.union(data_shopee_bc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "virgin-badge",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[customer_id: string, lst_items: array<string>, len_item: int]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_item_total.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "confirmed-clause",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fp_train_model(train_data):\n",
    "    fpGrowth = FPGrowth(itemsCol=\"lst_items\", minSupport=0.001, minConfidence=0.006)\n",
    "    model_fp = fpGrowth.fit(data_item_total)\n",
    "    return model_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "animated-thermal",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_user_potential = data_item_total.join(user_potential_pred,['customer_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "coupled-dressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model_fp.transform(data_user_potential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "local-billy",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o6640.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 13636.0 failed 4 times, most recent failure: Lost task 7.3 in stage 13636.0 (TID 382766) (10.233.88.16 executor 43): org.apache.spark.SparkException: Failed to execute user defined function(ClassificationModel$$Lambda$4321/0x00000008417fe840: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage56.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:774)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 16, y.size = 17\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:115)\n\tat org.apache.spark.ml.classification.LinearSVCModel.$anonfun$margin$1(LinearSVC.scala:346)\n\tat org.apache.spark.ml.classification.LinearSVCModel.$anonfun$margin$1$adapted(LinearSVC.scala:345)\n\tat org.apache.spark.ml.classification.LinearSVCModel.predictRaw(LinearSVC.scala:376)\n\tat org.apache.spark.ml.classification.LinearSVCModel.predictRaw(LinearSVC.scala:329)\n\tat org.apache.spark.ml.classification.ClassificationModel.$anonfun$transform$1(Classifier.scala:213)\n\t... 15 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function(ClassificationModel$$Lambda$4321/0x00000008417fe840: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage56.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:774)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\t... 1 more\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 16, y.size = 17\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:115)\n\tat org.apache.spark.ml.classification.LinearSVCModel.$anonfun$margin$1(LinearSVC.scala:346)\n\tat org.apache.spark.ml.classification.LinearSVCModel.$anonfun$margin$1$adapted(LinearSVC.scala:345)\n\tat org.apache.spark.ml.classification.LinearSVCModel.predictRaw(LinearSVC.scala:376)\n\tat org.apache.spark.ml.classification.LinearSVCModel.predictRaw(LinearSVC.scala:329)\n\tat org.apache.spark.ml.classification.ClassificationModel.$anonfun$transform$1(Classifier.scala:213)\n\t... 15 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-165-e9c5472910e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \"\"\"\n\u001b[1;32m    483\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o6640.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 13636.0 failed 4 times, most recent failure: Lost task 7.3 in stage 13636.0 (TID 382766) (10.233.88.16 executor 43): org.apache.spark.SparkException: Failed to execute user defined function(ClassificationModel$$Lambda$4321/0x00000008417fe840: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage56.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:774)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 16, y.size = 17\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:115)\n\tat org.apache.spark.ml.classification.LinearSVCModel.$anonfun$margin$1(LinearSVC.scala:346)\n\tat org.apache.spark.ml.classification.LinearSVCModel.$anonfun$margin$1$adapted(LinearSVC.scala:345)\n\tat org.apache.spark.ml.classification.LinearSVCModel.predictRaw(LinearSVC.scala:376)\n\tat org.apache.spark.ml.classification.LinearSVCModel.predictRaw(LinearSVC.scala:329)\n\tat org.apache.spark.ml.classification.ClassificationModel.$anonfun$transform$1(Classifier.scala:213)\n\t... 15 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function(ClassificationModel$$Lambda$4321/0x00000008417fe840: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage56.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:774)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\t... 1 more\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 16, y.size = 17\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:115)\n\tat org.apache.spark.ml.classification.LinearSVCModel.$anonfun$margin$1(LinearSVC.scala:346)\n\tat org.apache.spark.ml.classification.LinearSVCModel.$anonfun$margin$1$adapted(LinearSVC.scala:345)\n\tat org.apache.spark.ml.classification.LinearSVCModel.predictRaw(LinearSVC.scala:376)\n\tat org.apache.spark.ml.classification.LinearSVCModel.predictRaw(LinearSVC.scala:329)\n\tat org.apache.spark.ml.classification.ClassificationModel.$anonfun$transform$1(Classifier.scala:213)\n\t... 15 more\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "packed-congo",
   "metadata": {},
   "source": [
    "# Spark Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "identified-newman",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
